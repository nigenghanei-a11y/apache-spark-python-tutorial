{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51c689a0",
   "metadata": {},
   "source": [
    "# Apache Spark with Python (PySpark)\n",
    "## Short Tutorial\n",
    "\n",
    "This notebook is designed for a **short session** in the *Big Data & Business Intelligence* course.\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "By the end of this notebook, you should be able to:\n",
    "\n",
    "1. Explain what Apache Spark is and when to use it.\n",
    "2. Start a local PySpark session (`SparkSession`).\n",
    "3. Load data into Spark DataFrames.\n",
    "4. Perform basic transformations and aggregations.\n",
    "5. Use Spark SQL to query data.\n",
    "6. Build a simple ETL pipeline that reads CSV, transforms, joins, and writes Parquet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c7aee3",
   "metadata": {},
   "source": [
    "## 1. What is Apache Spark?\n",
    "\n",
    "Apache Spark is a **distributed data processing framework**. It is designed for:\n",
    "\n",
    "- Handling **large datasets** that do not fit into memory on one machine.\n",
    "- Running computations **in parallel** across a cluster of machines.\n",
    "- Providing a **high-level API** (DataFrames, SQL) that feels similar to Pandas or SQL.\n",
    "\n",
    "### Understanding the Problem: Why Do We Need Spark?\n",
    "\n",
    "Imagine you work for a company that has **terabytes** of customer data like transaction logs, clickstream data, sensor readings, etc. You need to analyze this data to generate reports or train machine learning models.\n",
    "\n",
    "**The Challenge:**\n",
    "- Your laptop has maybe 8-16 GB of RAM.\n",
    "- Traditional tools like **Pandas** load all data into memory on a single machine.\n",
    "- If your dataset is 500 GB, Pandas simply **cannot handle it**—your computer will crash or freeze.\n",
    "\n",
    "**The Solution: Distributed Computing**\n",
    "\n",
    "This is where **Apache Spark** comes in. Instead of trying to process all the data on one machine, Spark:\n",
    "\n",
    "1. **Splits** the data into smaller chunks.\n",
    "2. **Distributes** these chunks across multiple machines (called a \"cluster\").\n",
    "3. **Processes** each chunk in parallel on different machines.\n",
    "4. **Combines** the results back together.\n",
    "\n",
    "This approach allows you to process datasets that are **much larger than the memory of any single machine**.\n",
    "\n",
    "### Why not just use Pandas?\n",
    "\n",
    "| **Pandas** | **Spark** |\n",
    "|------------|-----------|\n",
    "| Keeps all data in **memory on a single machine** | **Distributes** data across many machines |\n",
    "| Fast for small-to-medium datasets (< 10 GB) | Designed for **large datasets** (GBs to PBs) |\n",
    "| Single-threaded or limited parallelism | Runs computations **in parallel** across all cores/machines |\n",
    "| Runs only on your laptop | Can run locally **or** on a cloud cluster with hundreds of machines |\n",
    "\n",
    "### When Should You Use Spark?\n",
    "\n",
    "- **Large datasets** that don't fit in memory (> 10-100 GB)\n",
    "- **Complex data pipelines** (ETL: Extract, Transform, Load)\n",
    "- **Big data analytics** and reporting\n",
    "- **Machine learning** on large datasets\n",
    "- When you need to **scale** from your laptop to a cloud cluster\n",
    "\n",
    "### When Should You Use Pandas Instead?\n",
    "\n",
    "- Dataset fits comfortably in memory (< 5 GB)\n",
    "- Quick exploratory analysis\n",
    "- Simple data manipulations\n",
    "\n",
    "### Key Concepts in Spark\n",
    "\n",
    "**1. Cluster Computing**\n",
    "- A **cluster** is a group of computers (called \"nodes\") working together.\n",
    "- Spark can run on a single machine (local mode) or on a cluster (distributed mode).\n",
    "- The same Spark code runs in **both modes**—no changes needed!\n",
    "\n",
    "**2. Lazy Evaluation**\n",
    "- Spark doesn't execute operations immediately.\n",
    "- It builds a **plan** of what to do, then executes everything when you ask for results.\n",
    "- This allows Spark to **optimize** the entire workflow.\n",
    "\n",
    "**3. DataFrames and SQL**\n",
    "- In modern Spark, we work with **DataFrames** (similar to Pandas DataFrames or SQL tables).\n",
    "- You can use **SQL queries** or **DataFrame API** methods—whichever you prefer.\n",
    "- Under the hood, Spark converts everything to optimized execution plans.\n",
    "\n",
    "### Real-World Example\n",
    "\n",
    "**Scenario:** An e-commerce company analyzes 2 TB of daily clickstream data.\n",
    "\n",
    "- **With Pandas:** Impossible—data doesn't fit in memory.\n",
    "- **With Spark:** \n",
    "  - Data is split across 100 machines.\n",
    "  - Each machine processes 20 GB in parallel.\n",
    "  - Results are combined to generate insights in minutes.\n",
    "  - The same code can run on your laptop with a sample dataset for testing!\n",
    "\n",
    "---\n",
    "\n",
    "In this tutorial, we will run Spark in **local mode** on your laptop, but remember: the same code can scale to process massive datasets in the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9fa5e0",
   "metadata": {},
   "source": [
    "## 2. Environment Setup — Starting a SparkSession\n",
    "\n",
    "> If you are running this locally and do not have PySpark installed yet, either run:\n",
    "\n",
    "```bash\n",
    "pip install pyspark\n",
    "```\n",
    "\n",
    "or follow the environment setup that uses the requirements.txt file to install pyspark.\n",
    "In this notebook, we create a **SparkSession**, which is the main entry point for using Spark with DataFrames and SQL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735ea68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(\"JAVA_HOME:\", os.environ.get(\"JAVA_HOME\"))\n",
    "print(\"PATH:\", os.environ.get(\"PATH\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2fa18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BD_BI_Spark_Tutorial\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Suppress verbose logging output\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "spark, spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bb2096",
   "metadata": {},
   "source": [
    "### Understanding the SparkSession Configuration\n",
    "\n",
    "Let's break down what each part of the SparkSession setup means:\n",
    "\n",
    "- **`SparkSession.builder`**: Starts the configuration process for creating a Spark session.\n",
    "- **`.appName(\"BD_BI_Spark_Tutorial\")`**: Sets a name for your application. This name appears in Spark's monitoring UI and logs, making it easy to identify your job.\n",
    "- **`.master(\"local[*]\")`**: Tells Spark where to run:\n",
    "  - `local` means run on your laptop (not a cluster)\n",
    "  - `[*]` means use all available CPU cores for parallel processing\n",
    "  - You could also use `local[2]` to use only 2 cores, or `local[4]` for 4 cores\n",
    "- **`.getOrCreate()`**: Creates a new SparkSession, or reuses an existing one if it's already running (useful when re-running cells).\n",
    "- **`.sparkContext.setLogLevel(\"ERROR\")`**: Reduces verbose output by only showing error messages.\n",
    "\n",
    "**Important:** Once created, you use the `spark` object to read data, create DataFrames, and run SQL queries throughout your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24db213e",
   "metadata": {},
   "source": [
    "## 3. Creating a Sample Dataset\n",
    "\n",
    "For this tutorial, we will create a small **housing dataset** directly in the notebook, instead of reading from disk.\n",
    "In a real setting, you would typically read from CSV, Parquet, or a database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcde20c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "data = [\n",
    "    Row(house_id=1, neighborhood_id=10, price=300000, sqft=80,  bedrooms=2, year_built=1990),\n",
    "    Row(house_id=2, neighborhood_id=10, price=450000, sqft=100, bedrooms=3, year_built=2005),\n",
    "    Row(house_id=3, neighborhood_id=11, price=500000, sqft=120, bedrooms=4, year_built=2010),\n",
    "    Row(house_id=4, neighborhood_id=11, price=200000, sqft=60,  bedrooms=2, year_built=1980),\n",
    "    Row(house_id=5, neighborhood_id=12, price=800000, sqft=150, bedrooms=5, year_built=2018),\n",
    "]\n",
    "\n",
    "houses_df = spark.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd27e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following lines to see the schema and data\n",
    "# houses_df.printSchema()\n",
    "# houses_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a89e7f",
   "metadata": {},
   "source": [
    "### DataFrame vs Pandas\n",
    "\n",
    "- A **Spark DataFrame** looks similar to a Pandas DataFrame when you `.show()` it.\n",
    "- But it is **not** actually held entirely in memory like in Pandas.\n",
    "- Spark only runs computations **lazily**: it plans the query first and only executes when needed (e.g., on `.show()`, `.count()`, `.collect()`).\n",
    "\n",
    "### Working with Large Datasets\n",
    "\n",
    "When working with very large datasets (millions or billions of rows), you should **never** try to see all the data at once. Here are the safe ways to inspect your data:\n",
    "\n",
    "**Preview the first few rows:**\n",
    "```python\n",
    "houses_df.show(5)          # Show first 5 rows (default is 20)\n",
    "houses_df.show(10, False)  # Show 10 rows without truncating wide columns\n",
    "```\n",
    "\n",
    "**Similar to Pandas `.head()`:**\n",
    "```python\n",
    "houses_df.limit(5).show()  # Equivalent to df.head(5) in Pandas\n",
    "```\n",
    "\n",
    "**What's the difference between `.show(5)` and `.limit(5).show()`?**\n",
    "\n",
    "- **`.show(5)`**: Only displays 5 rows but the DataFrame itself (`houses_df`) remains unchanged with all rows.\n",
    "- **`.limit(5).show()`**: Creates a **new DataFrame** with only 5 rows, then displays it. This new DataFrame can be saved and reused.\n",
    "\n",
    "```python\n",
    "# Example:\n",
    "top5 = houses_df.limit(5)  # Creates a new DataFrame with 5 rows\n",
    "top5.count()               # Returns 5\n",
    "houses_df.count()          # Still returns the original row count\n",
    "\n",
    "# .show(5) is just for display - it doesn't create a new DataFrame\n",
    "```\n",
    "\n",
    "Use `.show(n)` when you just want to peek at the data. Use `.limit(n)` when you need a smaller DataFrame for testing or further processing.\n",
    "\n",
    "**WARNING:** Never use `.collect()` on large datasets! This pulls ALL data into memory on your laptop and will crash it.\n",
    "```python\n",
    "# DANGEROUS with big data:\n",
    "# all_data = houses_df.collect()  # DON'T DO THIS with TB of data!\n",
    "```\n",
    "\n",
    "**Safe alternatives:**\n",
    "- Use `.show(n)` to preview a few rows\n",
    "- Use `.count()` to get the total number of rows\n",
    "- Use `.describe().show()` to get summary statistics\n",
    "- Use aggregations (`.groupBy()`, `.agg()`) to summarize data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca68af2d",
   "metadata": {},
   "source": [
    "### Exercise: Inspect the data\n",
    "\n",
    "1. Use `houses_df.count()` to count the rows.\n",
    "2. Use `houses_df.describe().show()` to see basic statistics.\n",
    "3. Try `houses_df.select(\"price\", \"sqft\").show()`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e2c6bd",
   "metadata": {},
   "source": [
    "## 4. Basic DataFrame Operations\n",
    "\n",
    "In this section, we will:\n",
    "\n",
    "- Select and filter columns.\n",
    "- Create new columns with `withColumn`.\n",
    "- Understand lazy evaluation briefly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4547a6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Add a new column: price per square meter\n",
    "houses_df = houses_df.withColumn(\"price_per_sqft\", col(\"price\") / col(\"sqft\"))\n",
    "\n",
    "houses_df.select(\"house_id\", \"price\", \"sqft\", \"price_per_sqft\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cf33f0",
   "metadata": {},
   "source": [
    "We can also filter rows using `.filter()` or `.where()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c7e4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for houses with more than 3 bedrooms\n",
    "large_houses = houses_df.filter(col(\"bedrooms\") > 3)\n",
    "\n",
    "large_houses.select(\"house_id\", \"bedrooms\", \"price\", \"sqft\", \"price_per_sqft\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2f4378",
   "metadata": {},
   "source": [
    "### Exercise: Create a custom metric\n",
    "\n",
    "1. Create a new column `price_per_bedroom` = `price / bedrooms`.\n",
    "2. Filter for houses where `price_per_bedroom` is less than **200,000**.\n",
    "3. Show the `house_id`, `bedrooms`, and `price_per_bedroom` columns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea3aa08",
   "metadata": {},
   "source": [
    "## 5. Aggregations with `groupBy`\n",
    "\n",
    "Spark makes it easy to compute statistics over groups, similar to `GROUP BY` in SQL or `.groupby()` in Pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a32464d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, min, max\n",
    "\n",
    "# Average price per number of bedrooms\n",
    "agg_df = houses_df.groupBy(\"bedrooms\").agg(\n",
    "    avg(\"price\").alias(\"avg_price\"),\n",
    "    min(\"price\").alias(\"min_price\"),\n",
    "    max(\"price\").alias(\"max_price\")\n",
    ").orderBy(\"bedrooms\")\n",
    "\n",
    "agg_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c79d57b",
   "metadata": {},
   "source": [
    "### Exercise: Aggregation practice\n",
    "\n",
    "1. Compute the **average** `price_per_sqft` per number of bedrooms.\n",
    "2. Sort the result by `price_per_sqft` in **descending** order.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5644d6",
   "metadata": {},
   "source": [
    "## 6. Spark SQL\n",
    "\n",
    "You can use **SQL queries** on Spark DataFrames. First, you need to register a DataFrame as a **temporary view**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d558ba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a temporary SQL view\n",
    "houses_df.createOrReplaceTempView(\"houses\")\n",
    "\n",
    "# Use SQL to compute average price per bedroom\n",
    "sql_result = spark.sql(\"\"\"\n",
    "SELECT bedrooms,\n",
    "       AVG(price) AS avg_price,\n",
    "       AVG(price_per_sqft) AS avg_price_per_sqft\n",
    "FROM houses\n",
    "GROUP BY bedrooms\n",
    "ORDER BY avg_price DESC\n",
    "\"\"\")\n",
    "\n",
    "sql_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88af2686",
   "metadata": {},
   "source": [
    "Spark SQL is useful because:\n",
    "\n",
    "- Many people already know **SQL**, so they can work with data quickly.\n",
    "- The same query can run on **very large** datasets in a cluster.\n",
    "- Spark can automatically optimize SQL queries internally.\n",
    "\n",
    "### Exercise: Your own SQL query\n",
    "\n",
    "Write a SQL query that:\n",
    "\n",
    "1. Selects `house_id`, `price`, `bedrooms`, `price_per_sqft`.\n",
    "2. Only keeps rows where `bedrooms >= 3`.\n",
    "3. Orders the results by `price_per_sqft` **ascending**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc54909",
   "metadata": {},
   "source": [
    "## 7. Joins: Combining Multiple Tables\n",
    "\n",
    "Let's imagine we have a second table that contains information about neighborhoods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe59d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small neighborhoods DataFrame\n",
    "neighborhood_data = [\n",
    "    Row(neighborhood_id=10, name=\"Central\", city=\"Metropolis\"),\n",
    "    Row(neighborhood_id=11, name=\"Northside\", city=\"Metropolis\"),\n",
    "    Row(neighborhood_id=12, name=\"Lakeside\", city=\"Springfield\"),\n",
    "]\n",
    "\n",
    "neigh_df = spark.createDataFrame(neighborhood_data)\n",
    "\n",
    "neigh_df.printSchema()\n",
    "neigh_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ab65a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join houses with neighborhoods on neighborhood_id\n",
    "joined_df = houses_df.join(neigh_df, on=\"neighborhood_id\", how=\"left\")\n",
    "\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45379960",
   "metadata": {},
   "source": [
    "### Exercise: Neighborhood statistics\n",
    "\n",
    "Using `joined_df`:\n",
    "\n",
    "1. Compute the **average house price** per neighborhood `name`.\n",
    "2. Compute the average `price_per_sqft` per **city**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ed8948",
   "metadata": {},
   "source": [
    "## 8. A Mini ETL Pipeline in Spark\n",
    "\n",
    "We now put the pieces together into a simple **ETL (Extract–Transform–Load)** function:\n",
    "\n",
    "- **Extract:** Read data (in this tutorial we reuse the DataFrames created earlier).\n",
    "- **Transform:** Clean data, add derived columns, join tables.\n",
    "- **Load:** Write the result to disk (e.g., as Parquet).\n",
    "\n",
    "**Note:** For simplicity, this tutorial uses the in-memory sample data (`houses_df` and `neigh_df`) we created earlier. In a real-world scenario, you would read data from files using `spark.read.csv()` or `spark.read.parquet()`, transform it, and write the results back to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a96d6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "def transform_housing_data(spark):\n",
    "    \"\"\"Example ETL function for housing data.\n",
    "\n",
    "    In a real project, you would:\n",
    "    - Read houses from CSV or a database.\n",
    "    - Read neighborhoods from another source.\n",
    "    - Clean, transform, and join them.\n",
    "    - Write out the final DataFrame to Parquet for downstream BI tools.\n",
    "    \"\"\"\n",
    "\n",
    "    # Here we reuse houses_df and neigh_df from the notebook scope.\n",
    "    # In a standalone script, you would read them inside this function.\n",
    "    global houses_df, neigh_df\n",
    "\n",
    "    df = houses_df.dropna(subset=[\"price\", \"sqft\"])  # simple cleaning\n",
    "\n",
    "    df = df.withColumn(\"price_per_sqft\", col(\"price\") / col(\"sqft\"))\n",
    "    df = df.withColumn(\"price_per_bedroom\", col(\"price\") / col(\"bedrooms\"))\n",
    "\n",
    "    result = df.join(neigh_df, on=\"neighborhood_id\", how=\"left\")\n",
    "\n",
    "    # In a real setting, uncomment to write to disk:\n",
    "    # result.write.mode(\"overwrite\").parquet(\"output/cleaned_housing\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "etl_df = transform_housing_data(spark)\n",
    "etl_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44a5bc6",
   "metadata": {},
   "source": [
    "### Exercise: Extend the ETL\n",
    "\n",
    "1. Add a new column `is_new` that is `True` if `year_built >= 2010`, else `False`.\n",
    "2. Aggregate `etl_df` to compute the average price for `is_new = True` vs `False`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9d01fe",
   "metadata": {},
   "source": [
    "## 9. Wrap-Up\n",
    "\n",
    "In this notebook, you:\n",
    "\n",
    "- Started a **SparkSession** in local mode.\n",
    "- Created Spark **DataFrames** and inspected their schema and contents.\n",
    "- Performed **basic transformations** and **aggregations** using the DataFrame API.\n",
    "- Used **Spark SQL** queries on DataFrames.\n",
    "- Performed **joins** to combine multiple tables.\n",
    "- Built a small **ETL function** that transforms and combines data.\n",
    "\n",
    "### Where to go from here\n",
    "\n",
    "- Try reading real CSV or Parquet files with `spark.read.csv(...)` or `spark.read.parquet(...)`.\n",
    "- Experiment with **larger datasets** to see the benefit of Spark over Pandas.\n",
    "- Look into **Spark MLlib** for scalable machine learning.\n",
    "\n",
    "When you close your notebook, you can stop Spark with:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b796a8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb84e18d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## BONUS: Reading and Writing Files\n",
    "\n",
    "This section demonstrates how to read from CSV and write to Parquet files. **These cells are optional and can be deleted if you don't need file I/O examples.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30875984",
   "metadata": {},
   "source": [
    "### Writing to Parquet\n",
    "\n",
    "Parquet is a columnar storage format that's efficient for big data. Here's how to save your results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdbd316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Writing to Parquet format\n",
    "# etl_df.write.mode(\"overwrite\").parquet(\"../data/tmp/cleaned_housing_parquet\")\n",
    "\n",
    "# To read it back:\n",
    "# df_from_parquet = spark.read.parquet(\"../data/tmp/cleaned_housing_parquet\")\n",
    "# df_from_parquet.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cf470d",
   "metadata": {},
   "source": [
    "### Reading from CSV\n",
    "\n",
    "Here's how you would read data from a CSV file in a real project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00107dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Reading a CSV file\n",
    "# df_from_csv = spark.read.csv(\n",
    "#     \"../data/your_file.csv\",\n",
    "#     header=True,        # First row contains column names\n",
    "#     inferSchema=True    # Automatically detect column types\n",
    "# )\n",
    "# df_from_csv.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
